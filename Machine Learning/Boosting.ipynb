{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Boosting__\n",
    "잘 못 맞춘 데이터를 더 잘 맞추도록 하기 위한 기법으로   \n",
    "오분류된 데이터에 초점을 맞추어 더 많은 가중치를 주는 방식\n",
    "\n",
    "복원 추출을 하여 샘플링 할 때  \n",
    "가중치가 많은 오분류된 데이터를 많이 뽑게 되고  \n",
    "가중치가 적은 정분류된 데이터를 적게 뽑게 된다.  \n",
    "\n",
    "이 커널에서 알아 볼  \n",
    "__Boosting__ 기법은 두가지 입니다.  \n",
    "- AdaBoost  \n",
    "- Gradient Boosting  \n",
    "\n",
    "\n",
    "## AdaBoost(Adaptive Boost)\n",
    "__Boosting__ 기법의 기본이 되는 모델로  \n",
    "오분류된 데이터에 더 많은 가중치를 주고  \n",
    "정분류된 데이터에는 적은 가중치를 둬    \n",
    "이 후에 복원추출을 할 때 오분류 데이터를 더 많이 뽑고    \n",
    "정분류된 데이터를 적게 뽑는 방법을 계속해서 반복하는 알고리즘  \n",
    "\n",
    "__Bagging__ 기법은 가중치와 관계없이  \n",
    "랜덤 복원 샘플링을 하므로 __Boosting__ 보다는 과적합이 적게 일어 난다.  \n",
    "\n",
    "__AdaBoost__의 경우 분류가 잘 되지 않은 샘플이 더 많이 뽑혀  \n",
    "학습을 시킬 경우 과적합의 위험이 존재한다.  \n",
    "\n",
    "## Gradient Boosting  \n",
    "__Gradient Boosting__의 경우   \n",
    "먼저, 데이터에 첫 학습기를 적합 후 발생하는 오차를 구한다.   \n",
    "\n",
    "> 여기서의 오차는 노이즈(예측 불가능한 오차)가 아닌  \n",
    "> 실제갑과 예측값의 차이인 잔차를 말한다.  \n",
    "\n",
    "\n",
    "다음으로 첫 학습기에서 발생한 오차를 예측하는 학습기를 만든다.  \n",
    "이후, 학습기에서 발생된 오차를 구한다.  \n",
    "\n",
    "다음으로 위 모델에서 발생한 오차를 예측하는 학습기를 만들고  \n",
    "발생한 오차를 구한다.  \n",
    "\n",
    "이 방법을 수없이 반복해 오차가 0에 가까워지게 만든다.  \n",
    "\n",
    "이후, 만들어진 학습기들을 결합시켜 하나의 학습기를 만든다. \n",
    "\n",
    "이를 __Gradient Boosting__이라 한다. \n",
    "\n",
    "\n",
    "__Gradient Boosting__ 종류\n",
    "* XGBoost\n",
    "* LightGBM\n",
    "* Catboost\n",
    "\n",
    "###  XGBoost\n",
    "__XGBoost__ 는 __Gradient Boost__ 에서 \n",
    "모델의 과적합을 방지하기 위해  \n",
    "`Regularization term`을 추가한 모델\n",
    "\n",
    "### LightGBM\n",
    "__XGBoost__ 는 트리를 학습시킬 때  \n",
    "하나의 레벨단위(Level-wise growth)로 진행하는 반면  \n",
    "\n",
    "__LightGBM__은 트리를 학습시킬 때  \n",
    "잎 단위로(Leaf-wise growh) 진행해  \n",
    "__XGBoost__보다 더 잘 예측가능하지만  \n",
    "과적합의 위험이 더 높아진다.  \n",
    "\n",
    "\n",
    "### CatBoost\n",
    "범주형 변수들에 대해 잘 예측하는 알고리즘으로  \n",
    "잔차의 분산을 최소화 하면서 bias를 피하는 boosting 기법이다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "- [올인원 패키지 : 머신러닝과 데이터분석 A-Z\n",
    "](https://www.fastcampus.co.kr/data_online_dataadv/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
