{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Boosting__\n",
    "잘 못 맞춘 데이터를 더 잘 맞추도록 하기 위한 기법으로   \n",
    "오분류된 데이터에 초점을 맞추어 더 많은 가중치를 주는 방식\n",
    "\n",
    "복원 추출을 하여 샘플링 할 때  \n",
    "가중치가 많은 오분류된 데이터를 많이 뽑고  \n",
    "가중치가 적은 정분류된 데이터를 적게 뽑습니다.  \n",
    "\n",
    "__Bagging__과의 차이점은  \n",
    "> 가중치 부과 여부  \n",
    "> __Bagging__ 병렬 학습, __Boosting__ 순차 학습\n",
    "\n",
    "![diff](./image/Boosting/diff.png)\n",
    "\n",
    "이 커널에서 알아 볼  \n",
    "__Boosting__ 기법은 두가지 입니다.  \n",
    "- AdaBoost  \n",
    "- Gradient Boosting  \n",
    "\n",
    "\n",
    "## AdaBoost(Adaptive Boost)\n",
    "__Boosting__ 기법의 기본이 되는 모델로  \n",
    "오분류된 데이터에 더 많은 가중치를 주고  \n",
    "정분류된 데이터에는 적은 가중치를 둬    \n",
    "\n",
    "복원추출시 오답 데이터를 많이 뽑고    \n",
    "정답 데이터를 적게 뽑아 모델 학습을 반복하는 알고리즘  \n",
    "\n",
    "### __모델 학습 방법__\n",
    "1. 샘플링 후 모델 학습  \n",
    "2. 정답/오답 데이터에 낮은/높은 가중치 부여  \n",
    "3. 복원 추출\n",
    "4. 학습 후 가중치 갱신\n",
    "5. 위의 과정을 반복  \n",
    "\n",
    "## Gradient Boosting  \n",
    "__Gradient Boosting__은 __AdaBoost__와 비슷하지만,  \n",
    "가충치 부여 방식에서 차이가 존재합니다.  \n",
    "\n",
    "__Gradient Boosting__는     \n",
    "모델의 오차가 작아지는 방향으로 가중치를 부여합니다.  \n",
    "\n",
    "> 여기서의 오차는 노이즈(예측 불가능한 오차)가 아닌  \n",
    "> 실제 값과 예측 값의 차이인 잔차를 말합니다.  \n",
    "\n",
    "### __모델 학습 방법__\n",
    "1. 샘플링 후 모델 학습  \n",
    "2. 모델의 오차 계산    \n",
    "3. 오차를 예측하는 학습기 생성 및 학습  \n",
    "4. 위의 과정을 반복  \n",
    "\n",
    "__Gradient Boosting__ 종류\n",
    "* XGBoost\n",
    "* LightGBM\n",
    "* Catboost\n",
    "\n",
    "###  XGBoost\n",
    "__XGBoost__ 는 __Gradient Boost__ 에서 \n",
    "모델의 과적합을 방지하기 위해  \n",
    "`Regularization term`을 추가한 모델\n",
    "\n",
    "### LightGBM\n",
    "__XGBoost__ 는 트리를 학습시킬 때  \n",
    "균형트리 분할(Level-wise)로 진행하는 반면  \n",
    "\n",
    "__LightGBM__은 트리를 학습시킬 때  \n",
    "리프중심 트리 분할(Leaf-wise)로 진행 \n",
    "\n",
    "> __균형 트리 분할__\n",
    "> - 레벨별로 균형있게 트리를 학습시키고, 트리의 깊이를 최소화 함   \n",
    "> - 과적합 위험이 적지만 모델 생성에 시간이 드는 편\n",
    "\n",
    "> __리프중심 트리 분할__\n",
    "> - 최대 손실 값을 가지는 리프노드를 지속적으로 분할  \n",
    "> - 과적합 위험이 높지만 모델 생성이 빠른 편\n",
    "\n",
    "![tree_diff](./image/Boosting/tree_diff.png)\n",
    "\n",
    "### CatBoost\n",
    "범주형 변수들에 최적화된 알고리즘으로  \n",
    "__XGBoost__와 같이 균형트리 분할로 진행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "### Image\n",
    "- [What's the difference between boosting and bagging?](https://www.quora.com/Whats-the-difference-between-boosting-and-bagging)\n",
    "- [LightGBMの解説](http://data-analysis-stats.jp/2019/11/13/lightgbm%E3%81%AE%E8%A7%A3%E8%AA%AC/)\n",
    "\n",
    "### Boosting\n",
    "- [boosting 기법 이해 (bagging vs boosting)](https://www.slideshare.net/freepsw/boosting-bagging-vs-boosting)\n",
    "- [[Chapter 4. 분류] 부스팅알고리즘(AdaBoost, GBM)](https://injo.tistory.com/31)\n",
    "- [CatBoost](https://gentlej90.tistory.com/100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
